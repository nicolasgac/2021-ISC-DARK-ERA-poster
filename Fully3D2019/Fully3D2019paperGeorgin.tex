\documentclass[12pt]{spieman}  % 12pt font required by SPIE;
%\documentclass[twocolumn,12pt]{spieman}  % use this instead for A4 paper
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{tocloft}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{color,xspace}
\graphicspath{{figures_fully3d2019/}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
%\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
%\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\title{Multi-streaming and multi-GPU optimization for a matched pair of Projector and Backprojector}

\author[a]{Nicolas Georgin}
\author[a,b]{Camille Chapdelaine}
\author[b]{Nicolas Gac}
\author[c]{Ali Mohammad-Djafari}
\author[a]{Estelle Parra}
\affil[a]{Safran Tech, Signal and Information Technologies Department, Rue des Jeunes Bois, Ch\^ateaufort,78114 Magny-Les-Hameaux, France}
\affil[b]{Laboratoire des signaux et syst\`emes, CNRS, CentraleSup\'elec-Univ Paris Saclay, Gif-sur-Yvette, France}
\affil[c]{International Science Consulting \& Training, 91440 Bures-sur-Yvette, France}

\renewcommand{\cftdotsep}{\cftnodots}
\cftpagenumbersoff{figure}
\cftpagenumbersoff{table} 
\begin{document} 
\maketitle

\begin{abstract}
Iterative reconstruction methods are used in X-ray Computed Tomography in order to improve the quality of reconstruction compared to filtered backprojection methods. However, these methods are computationally expensive due to repeated projection and backprojection operations. Among the possible pairs of projector and backprojector, the Separable Footprint (SF) pair has the advantage to be matched in order to ensure the convergence of the reconstruction algorithm. Nevertheless, this pair implies more computations compared to unmatched pairs commonly used in order to reduce the computation time. In order to speed up this pair, the projector and the backprojector can be parallelized on GPU. Following one of our previous work, in this paper, we propose a new method which takes benefits from the factorized calculations of the SF pair in order to increase the number of data handled by each thread. We also describe the adaptation of this implementation for multi-streaming computations. The method is tested on large volumes of size $1024^3$ and $2048^3$ voxels.
\end{abstract}

% Include a list of up to six keywords after the abstract
\keywords{Computed Tomography, Seperable Footprint (SF), Graphical Computed Unit (GPU), CUDA}

% Include email contact information for corresponding author
{\noindent \footnotesize\textbf{*}Nicolas Georgin,  \linkable{nicolas.georgin@supelec.fr} }
{\noindent \footnotesize\textbf{*}Camille Chapdelaine,  \linkable{camille.chapdelaine@l2s.centralesupelec.fr} }

\begin{spacing}{1}   % use double spacing for rest of manuscript ??

\section{Introduction}
\label{sect:intro}


Iterative reconstruction methods can greatly improve the quality of reconstruction in cone-beam Computed Tomography (CBCT), particularly for low-dose acquisition\cite{long20103d}. Unfortunately, these methods suffer from a very long computation time which is conditioned by the speed of projection and backprojection operators. Theoretically, the backprojector is the adjoint operator of the projector. Nevertheless,  unmatched operators are very often used in order to alleviate the computational burden, especially for 3D reconstruction. This corresponds to an approximation which can hinder the convergence of the reconstruction algorithm\cite{arcadu2016crucial}. 

In order to ensure the convergence, computationally-efficient pairs have been proposed\cite{de2004distance, long20103d}. Among these pairs, the Separable Footprint (SF) pair\cite{long20103d} approximates the footprint of a voxel onto the detector as a separable function with respect to the transaxial and the axial directions. Thanks to this approximation, the computations can be factorized so SF projection and backprojection can be computed in a reasonable time. Nevertheless, this matched pair requires more computations than unmatched pairs, even parallelized on Graphical Processing Unit (GPU)\cite{wu2011gpu, xie2017accelerating, chapdelaine2018new}.

This paper describes how, based on our previous work\cite{chapdelaine2018new}, the SF projection and backprojection operators are accelerated using multi-streaming and multi-GPU computation. We propose a new design of the projector and backprojector kernels, where each thread computes the projection and backprojection of multiple cells and voxels respectively. We also present a way to hide data transfers which are known to be the main bottleneck for GPU computing. Next, we validate our new implementation on small volumes, then present results of acceleration on large volumes.


\section{Acceleration of the SF pair}
\label{sect:Matchedpair}


\subsection{Acceleration of the SF Projector}

%We consider a volume $\textbf{f}$ composed of $N_x \times N_y \times N_z$ cubic voxels, where we denoted $(x_e,y_e,z_e)$ the coordinates of the center of each voxel of side $\delta$.
%The plan-detector composed of $N_u \times N_v$ detectors, $(u_e,v_e)$ represents the center of each cell. A volume is put behind an X-ray source and the detector, to acquire projections, X-rays are sent on the volume,  rotating them by $\phi$ angle around the $z$-axis. The SF projection on a cell $(u_e,v_e)$ at an angle $\phi$ of the voxel $(x_e,y_e,z_e)$ is given by
The SF projection is given by\cite{chapdelaine2018new}
\begin{align}
 g(u_e,v_e,\phi) = & l_{\theta_c}(u_e,v_e) \sum_{x_e,y_e} l_{\phi_v}(\phi ; x_e,y_e) F_{trans} (u_e, \phi ;x_e,y_e) 
 \label{eq:SF projection} \\ & \sum_{z_{min}}^{z_{max}} F_{ax} (v_e,\phi;x_e,y_e,z_e) f(x_e,y_e,z_e) \nonumber
\end{align}
where $F_{trans} (u_e, \phi ;x_e,y_e)$ and $F_{ax} (v_e,\phi;x_e,y_e)$ are transaxial and axial footprints of the voxel, $ l_{\theta_c}(u_e,v_e)$ and $l_{\phi_v}(\phi ; x_e,y_e)$
are the amplitude functions given by A2 method \cite{chapdelaine2018new}. 
To avoid the conflict between the threads, the GPU implementation of the SF projector is ray-driven\cite{chapdelaine2018new}.

The original kernel\cite{chapdelaine2018new} has a lot of intermediate computations which are redundant from one kernel to another, due to the factorizations in formula \eqref{eq:SF projection}. In order to reduce these redundant computations, the kernel is modified so that it can process several cells per thread in the $v$-direction : this direction is chosen in order to factorize the computation of the transaxial footprint which requires more time than the axial footprint, since the shape of the transaxial footprint is trapezoidal  and the shape of the axial footprint is rectangular in the SF pair\cite{long20103d}. For this purpose, two arrays are created in shared memory in order to avoid to increase the number of registers in the kernel.
One array stores axial footprint corresponding to each cell, while the other array stores the product of transaxial and axial footprints. Then the projection is updated.

%Algorithm \ref{alg:Proposed SF projector} summaries the pseudo-code for the new implementation of SF projector.

\subsection{Acceleration of the SF Backprojector}
For a voxel $(x_e,y_e,z_e)$, the matched SF backprojection is given by\cite{long20103d}
\begin{align}
    b(x_e,y_e,z_e) = & \sum_{\phi} l_{\phi_{v}}(\phi; x_e,y_e) \sum_{v_{min}}^{v_{max}} F_{ax}(v_e, \phi ; x_e,y_e,z_e)   \\
    & \times  \sum_{u_{min}}^{u_{max}} l_{\theta_{c}}(u_e,v_e) F_{trans}(u_e, \phi; x_e,y_e) g(u_e,v_e, \phi) \nonumber
\end{align}
The original kernel\cite{chapdelaine2018new} computes the backprojection of a voxel  $(x_e,y_e,z_e)$. The kernel is modified so that one thread can process a column of voxels in the $z$-direction.  At one projection angle $\phi$, we store in shared memory all the transaxial footprints of each cell and pick-up for each voxel the transaxial footprints required  for the computation.

Then, for each voxel, the sum of the axial and transaxial products is stored in shared memory. The dimension of this array is equal the number of $z_e$ processed by each thread.
Algorithm \ref{alg:Proposed SF backprojector} summaries the pseudo-code of our implementation of the SF backprojector.

All the values of the arrays in our modified kernel are stored in the global memory which is a very slow memory. To optimize the access data, the arrays are stored in the shared memory and to guarentee a unique access, the dimension of the arrays are multiplied by the size of the block.


\begin{algorithm}
\caption{Proposed SF backprojector}
\label{alg:Proposed SF backprojector}
\begin{algorithmic}

\STATE Initialize $B_{back}[z_{e_0} : z_{e_0} + K -1]=0$
 \FOR{each $\phi$}
 \STATE Compute $v_{min}(z_{e_0})$ and $v_{max}(z_{e_0} + K - 1) $
 \STATE Initialize $\tilde{F}_{trans}[v_{min}(z_{e_0}): v_{min}(z_{e_0}+K-1)] =0$
 \FOR{$u_e = u_{min}$ to $u_{max}$}
   \STATE Compute $F_{trans}$
    \FOR{$v_e$ = $v_{min}(z_e)$ to $v_{max}(z_e + K - 1)$}
      \STATE $\Tilde{F}_{trans}[v_{e}]  += F_{trans}$  $l_{\theta_{c}}(u_e,v_e) g(u_e,v_e,\phi)$
   \ENDFOR
 \ENDFOR
 \FOR{$z_e = z_{e_0}$ to $z_{e_0} + K - 1$}
 \STATE Compute $v_{min}(z_e)$ and $v_{max}(z_e)$ and initialize $B_{\phi}$ = 0
 \FOR{$v_e = v_{min}(z)$ to $v_{max}(z)$}
        \STATE Compute and store in shared memory $F_{ax}[v_e]$
		\STATE $B_{\phi} += F_{ax}[v_e] \times F_{trans}[v_e]$ 
 \ENDFOR
 \STATE Compute and store in shared memory $B_{back}[z_e] += l_{\phi_{v}} \times B_{\phi}$ 
  \ENDFOR
 \FOR{$z_e = z_{e_0}$ to $z_{e_0} + K - 1$}
 \STATE $B(x_e,y_e,z_e) = B_{back}[z_e]$
 \ENDFOR
 \ENDFOR 

\end{algorithmic}
\end{algorithm}

\subsection{Acceleration with multi-streaming and multi-GPU}
For large volumes, it is not possible to copy all the data on the GPU.  Parts of the data are sent to the GPU one after the other and the results are collected by the CPU. Streams are used to make parallelism tasks in order to mask data transfer, so the GPUs send results and receive data during the computation of one block operator. Multi-GPU computations work following the same principle in order to further accelerate the operators.

\section{Results}
%We present the results of the two new implementations. %Firstly, 
The new kernel was used to reconstruct a volume of $256^3$ voxels, with a detector of $256^2$ cells and $256$ projections, using a NVIDIA TITAN V graphic card. Sizes of the projector and the backprojection were set to $16 \times 16 \times 1$. Table \ref{tab:voxelcell} shows the impact of the runtime operator depending on the number of cells or voxels by thread. We observed a three fold acceleration for the bacprojector operator and a two fold acceleration for the projector operator
%The runtime of the backprojector operator is accelereted by more than 3 while the projector can be accelerated by more than 2. 
The coupling degree introduced in \cite{arcadu2016crucial} of our GPU SF pair is equal to $1.0001$ which is very close to 1, which means our SF pair is very matched, furthermore the coupling degree of our SF pair does not depend on the number of voxels/cells.
\begin{table}[h]
\centering
 \begin{tabular}{||c c | c c||} 
  \hline
Number of Cells & Time Backprojector (ms) & Number of voxels &Time projector  (ms) \\
 \hline\hline
 1 & 367 & 1 & 999 \\ 
 \hline
 2 & 221 & 2 & 605\\
 \hline
 4 & 159 & 4 &\textbf{416} \\
 \hline
 8 & \textbf{109} & 8 &438\\ 
 \hline
  16 & 147 & 16 & 419 \\  
 \hline
\end{tabular}
    \caption{Runtime of the projection and backprojection operator depending on the number of cells or voxels}
    \label{tab:voxelcell}
\end{table}
The table \ref{tab:largevolume} presents the runtime of the new backprojector and projector using multi-GPU and multi-streams on large volume $1024^3$ and $2048^3$ voxels. We use TITAN X Maxwell GPU, each thread of our backprojector treats 8 voxels and the projector treats 16 cells, the size of the projector block is $128 \times 1 \times 1$ which is fully optimized and the backprojector is $16 \times 16 \times 1$. 
We observe of the runtime 
\begin{table}[h]
\centering
 \begin{tabular}{||l c c c c||} 
  \hline
 Operator &  1 Stream,1 GPU  & 4 Stream 1 GPU & 1 Streams 8 GPUs & 4 Streams 8 GPUs \\
 \hline\hline
 P 1024 voxels & 255686 ms & 255256 ms & 39084 ms & 38126 ms \\ 
 \hline
 BP 1024 voxels  & 95787 ms & 93490 ms & 18336 ms & 17929 ms \\
 \hline
 P 2048 voxels & 4260676 ms & 4254497 ms & 599496 ms & 584414 ms
 \\
 \hline
 BP 2048 voxels & 1530856 ms & 1522824 ms & 244798 ms & 233101 ms\\ 
 \hline
\end{tabular}
\label{tab:largevolume}
    \caption{Runtime of the projection (P) and backprojection (BP) operator depending on the number of streams and GPUs}
\end{table}

\section{Conclusion}

Multi-cell, multi-voxel, multi-GPU and  multi-stream accelerate the runtime of forward and back projector. The major factor of acceleration is number of GPUs followed by the number of voxels and cells. For small volumes, one GPU can be used. The number of stream provide marginal acceleration gains.


%%%%% References %%%%%

\bibliography{ChapdelaineBibliographie}   % bibliography data in report.bib
\bibliographystyle{spiejour}   % makes bibtex use spiejour.bst


\end{spacing}
\end{document}